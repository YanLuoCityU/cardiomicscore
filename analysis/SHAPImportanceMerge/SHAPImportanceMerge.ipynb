{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmicsNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Reads SHAP value files from multiple runs, calculates the element-wise average,\n",
    "displays a verification sample for a single data point, and saves the final averaged SHAP values.\n",
    "\"\"\"\n",
    "# 1. Define paths\n",
    "# Base path where the SEED directories (1234, 1235, etc.) are located\n",
    "base_path = \"/your path/cardiomicscore/saved/results/SHAP/OmicsNet/Proteomics\"\n",
    "\n",
    "# Output path for the final averaged results\n",
    "output_path = os.path.join(base_path, \"Final\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "\n",
    "# 2. Automatically discover SEED directories and outcome files\n",
    "try:\n",
    "    # Find all directories that are named like a number (our SEEDs)\n",
    "    seed_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.isdigit()])\n",
    "    if not seed_dirs:\n",
    "        raise FileNotFoundError(\"No valid SEED directories found in the base path.\")\n",
    "    \n",
    "    print(f\"Found {len(seed_dirs)} SEED runs: {seed_dirs}\")\n",
    "\n",
    "    # Get the list of SHAP files to process from the first SEED directory\n",
    "    first_seed_path = os.path.join(base_path, seed_dirs[0])\n",
    "    shap_files_to_process = [f for f in os.listdir(first_seed_path) if f.endswith('.parquet')]\n",
    "    if not shap_files_to_process:\n",
    "        raise FileNotFoundError(f\"No .parquet files found in directory: {first_seed_path}\")\n",
    "        \n",
    "    print(f\"Found {len(shap_files_to_process)} outcomes to process: {shap_files_to_process}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 3. Loop through each SHAP file name (e.g., 'shap_af.parquet')\n",
    "for shap_filename in shap_files_to_process:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing outcome file: {shap_filename}\")\n",
    "\n",
    "    list_of_dfs = []\n",
    "    # Collect the full path for the current file from each SEED directory\n",
    "    for seed in seed_dirs:\n",
    "        file_path = os.path.join(base_path, seed, shap_filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                list_of_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file {file_path}. Error: {e}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: File not found and will be skipped: {file_path}\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(f\"No valid data found for {shap_filename}. Skipping to next outcome.\")\n",
    "        continue\n",
    "        \n",
    "    # 4. Calculate the element-wise average of the SHAP values.\n",
    "    # This is the core calculation. It produces a DataFrame with the exact same\n",
    "    # dimensions (sample size, number of proteins) as the original files.\n",
    "    final_avg_df = sum(list_of_dfs) / len(list_of_dfs)\n",
    "    print(f\"Successfully averaged {len(list_of_dfs)} files. Resulting shape: {final_avg_df.shape}\")\n",
    "\n",
    "    print(\"\\n--- Verification for a random sample and 10 random proteins ---\")\n",
    "    \n",
    "    # Check if the dataframe is empty\n",
    "    if final_avg_df.empty:\n",
    "        print(\"Averaged DataFrame is empty, skipping verification.\")\n",
    "    else:\n",
    "        # Get the number of samples (rows) and proteins (columns)\n",
    "        num_samples, num_proteins = final_avg_df.shape\n",
    "\n",
    "        # Pick one random sample (person/row) to check\n",
    "        random_sample_index = random.randint(0, num_samples - 1)\n",
    "        print(f\"Verifying with data from a single random sample (row index: {random_sample_index})\")\n",
    "\n",
    "        # Pick 10 random proteins (columns) to check\n",
    "        all_proteins = final_avg_df.columns.tolist()\n",
    "        num_to_sample = min(10, num_proteins)\n",
    "        sample_proteins = random.sample(all_proteins, num_to_sample)\n",
    "        \n",
    "        verification_data = []\n",
    "        for protein in sample_proteins:\n",
    "            row_data = {'Protein': protein}\n",
    "            \n",
    "            # Get the specific raw SHAP value from each individual run for the selected sample and protein\n",
    "            for i, df in enumerate(list_of_dfs):\n",
    "                seed_name = seed_dirs[i]\n",
    "                # Use .iloc to get the value at the specific row/column position\n",
    "                row_data[f'Run_{seed_name}_SHAP'] = df.iloc[random_sample_index][protein]\n",
    "            \n",
    "            # Get the final averaged SHAP value for the same sample and protein\n",
    "            row_data['Final_Avg_SHAP'] = final_avg_df.iloc[random_sample_index][protein]\n",
    "            verification_data.append(row_data)\n",
    "        \n",
    "        verification_df = pd.DataFrame(verification_data)\n",
    "        # Use to_string() to ensure all columns are displayed properly\n",
    "        print(verification_df.to_string())\n",
    "    \n",
    "    # 6. Save the final averaged DataFrame. The structure is already correct.\n",
    "    output_file_path = os.path.join(output_path, shap_filename)\n",
    "    try:\n",
    "        final_avg_df.to_parquet(output_file_path, engine='pyarrow')\n",
    "        print(f\"\\nSuccessfully saved final average SHAP values to:\\n{output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving file to {output_file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_directory = '/your path/cardiomicscore/saved/results/SHAP/OmicsNet/Proteomics/Final'\n",
    "\"\"\"\n",
    "Iterates through 'shap_*.parquet' files in a specified directory, calculates\n",
    "the mean absolute SHAP values, and plots a bar chart of the top 30 proteins\n",
    "for each file onto a 2x3 grid.\n",
    "\n",
    "Args:\n",
    "    data_directory (str): The path to the data directory containing the SHAP parquet files.\n",
    "\"\"\"\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(data_directory):\n",
    "    print(f\"Error: Directory '{data_directory}' not found.\")\n",
    "\n",
    "# Get a list of all relevant parquet files\n",
    "parquet_files = [f for f in os.listdir(data_directory) if f.startswith('shap_') and f.endswith('.parquet')]\n",
    "\n",
    "# Ensure we have files to process\n",
    "if not parquet_files:\n",
    "    print(f\"No 'shap_*.parquet' files found in the directory '{data_directory}'.\")\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# 1. Create a 2x3 subplot grid. The figsize needs to be large enough for all charts.\n",
    "# fig is the entire figure window, axes is an array containing 6 subplot objects.\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(22, 14))\n",
    "\n",
    "# Flatten the 2D axes array into a 1D array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# --- Loop through files and plot ---\n",
    "for i, filename in enumerate(parquet_files):\n",
    "    # Stop if the number of files exceeds the number of subplots\n",
    "    if i >= 6:\n",
    "        print(\"Warning: More than 6 parquet files found. Only processing the first 6.\")\n",
    "        break\n",
    "\n",
    "    # Extract the outcome name from the filename\n",
    "    outcome_name = filename.replace('shap_', '').replace('.parquet', '')\n",
    "    full_path = os.path.join(data_directory, filename)\n",
    "    \n",
    "    print(f\"--- Processing: {filename} (Outcome: {outcome_name.upper()}) ---\")\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    try:\n",
    "        # Read the parquet file instead of csv\n",
    "        df = pd.read_parquet(full_path)\n",
    "        if 'eid' in df.columns:\n",
    "            df = df.drop(columns=['eid'])\n",
    "            print(\"  'eid' column removed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File '{full_path}' not found.\")\n",
    "        continue\n",
    "        \n",
    "    mean_abs_shap = df.abs().mean(axis=0)\n",
    "    top_30_shap = mean_abs_shap.sort_values(ascending=False).head(30)\n",
    "    \n",
    "    print(f\"  Calculated Top 30 proteins for {outcome_name.upper()}.\")\n",
    "\n",
    "    # --- Plot on the designated subplot ---\n",
    "    # 2. Select the current subplot to draw on\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # To display the most important proteins at the top, reverse the data order\n",
    "    top_30_shap.iloc[::-1].plot(kind='barh', ax=ax, color='c', zorder=2)\n",
    "    \n",
    "    # 3. Set the title and labels for each subplot\n",
    "    ax.set_title(f'Top 30 for {outcome_name.upper()}', fontsize=14, weight='bold')\n",
    "    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=10)\n",
    "    ax.set_ylabel('Protein', fontsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=8) # Adjust y-axis label font size\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7, zorder=1) # Add a grid for the x-axis\n",
    "\n",
    "# --- Post-processing ---\n",
    "# If there are fewer than 6 files, hide the remaining empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# 4. Add a main title for the entire figure\n",
    "fig.suptitle('Top 30 Proteins by Mean Absolute SHAP Value for Each CVD Outcome', fontsize=20, weight='bold')\n",
    "\n",
    "# 5. Automatically adjust the layout to prevent overlap\n",
    "# The rect parameter makes room for the main title (suptitle)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# 6. Save and display the final figure\n",
    "# You can uncomment the next line to save the figure to a file\n",
    "# plt.savefig('combined_shap_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabolomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Reads SHAP value files from multiple runs, calculates the element-wise average,\n",
    "displays a verification sample for a single data point, and saves the final averaged SHAP values.\n",
    "\"\"\"\n",
    "# 1. Define paths\n",
    "# Base path where the SEED directories (1234, 1235, etc.) are located\n",
    "base_path = \"/your path/cardiomicscore/saved/results/SHAP/OmicsNet/Metabolomics\"\n",
    "\n",
    "# Output path for the final averaged results\n",
    "output_path = os.path.join(base_path, \"Final\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "\n",
    "# 2. Automatically discover SEED directories and outcome files\n",
    "try:\n",
    "    # Find all directories that are named like a number (our SEEDs)\n",
    "    seed_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.isdigit()])\n",
    "    if not seed_dirs:\n",
    "        raise FileNotFoundError(\"No valid SEED directories found in the base path.\")\n",
    "    \n",
    "    print(f\"Found {len(seed_dirs)} SEED runs: {seed_dirs}\")\n",
    "\n",
    "    # Get the list of SHAP files to process from the first SEED directory\n",
    "    first_seed_path = os.path.join(base_path, seed_dirs[0])\n",
    "    shap_files_to_process = [f for f in os.listdir(first_seed_path) if f.endswith('.parquet')]\n",
    "    if not shap_files_to_process:\n",
    "        raise FileNotFoundError(f\"No .parquet files found in directory: {first_seed_path}\")\n",
    "        \n",
    "    print(f\"Found {len(shap_files_to_process)} outcomes to process: {shap_files_to_process}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 3. Loop through each SHAP file name (e.g., 'shap_af.parquet')\n",
    "for shap_filename in shap_files_to_process:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing outcome file: {shap_filename}\")\n",
    "\n",
    "    list_of_dfs = []\n",
    "    # Collect the full path for the current file from each SEED directory\n",
    "    for seed in seed_dirs:\n",
    "        file_path = os.path.join(base_path, seed, shap_filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                list_of_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file {file_path}. Error: {e}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: File not found and will be skipped: {file_path}\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(f\"No valid data found for {shap_filename}. Skipping to next outcome.\")\n",
    "        continue\n",
    "        \n",
    "    # 4. Calculate the element-wise average of the SHAP values.\n",
    "    # This is the core calculation. It produces a DataFrame with the exact same\n",
    "    # dimensions (sample size, number of proteins) as the original files.\n",
    "    final_avg_df = sum(list_of_dfs) / len(list_of_dfs)\n",
    "    print(f\"Successfully averaged {len(list_of_dfs)} files. Resulting shape: {final_avg_df.shape}\")\n",
    "\n",
    "    print(\"\\n--- Verification for a random sample and 10 random proteins ---\")\n",
    "    \n",
    "    # Check if the dataframe is empty\n",
    "    if final_avg_df.empty:\n",
    "        print(\"Averaged DataFrame is empty, skipping verification.\")\n",
    "    else:\n",
    "        # Get the number of samples (rows) and proteins (columns)\n",
    "        num_samples, num_proteins = final_avg_df.shape\n",
    "\n",
    "        # Pick one random sample (person/row) to check\n",
    "        random_sample_index = random.randint(0, num_samples - 1)\n",
    "        print(f\"Verifying with data from a single random sample (row index: {random_sample_index})\")\n",
    "\n",
    "        # Pick 10 random proteins (columns) to check\n",
    "        all_proteins = final_avg_df.columns.tolist()\n",
    "        num_to_sample = min(10, num_proteins)\n",
    "        sample_proteins = random.sample(all_proteins, num_to_sample)\n",
    "        \n",
    "        verification_data = []\n",
    "        for protein in sample_proteins:\n",
    "            row_data = {'Protein': protein}\n",
    "            \n",
    "            # Get the specific raw SHAP value from each individual run for the selected sample and protein\n",
    "            for i, df in enumerate(list_of_dfs):\n",
    "                seed_name = seed_dirs[i]\n",
    "                # Use .iloc to get the value at the specific row/column position\n",
    "                row_data[f'Run_{seed_name}_SHAP'] = df.iloc[random_sample_index][protein]\n",
    "            \n",
    "            # Get the final averaged SHAP value for the same sample and protein\n",
    "            row_data['Final_Avg_SHAP'] = final_avg_df.iloc[random_sample_index][protein]\n",
    "            verification_data.append(row_data)\n",
    "        \n",
    "        verification_df = pd.DataFrame(verification_data)\n",
    "        # Use to_string() to ensure all columns are displayed properly\n",
    "        print(verification_df.to_string())\n",
    "    \n",
    "    # 6. Save the final averaged DataFrame. The structure is already correct.\n",
    "    output_file_path = os.path.join(output_path, shap_filename)\n",
    "    try:\n",
    "        final_avg_df.to_parquet(output_file_path, engine='pyarrow')\n",
    "        print(f\"\\nSuccessfully saved final average SHAP values to:\\n{output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving file to {output_file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_directory = '/your path/cardiomicscore/saved/results/SHAP/OmicsNet/Metabolomics/Final'\n",
    "\"\"\"\n",
    "Iterates through 'shap_*.parquet' files in a specified directory, calculates\n",
    "the mean absolute SHAP values, and plots a bar chart of the top 30 proteins\n",
    "for each file onto a 2x3 grid.\n",
    "\n",
    "Args:\n",
    "    data_directory (str): The path to the data directory containing the SHAP parquet files.\n",
    "\"\"\"\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(data_directory):\n",
    "    print(f\"Error: Directory '{data_directory}' not found.\")\n",
    "\n",
    "# Get a list of all relevant parquet files\n",
    "parquet_files = [f for f in os.listdir(data_directory) if f.startswith('shap_') and f.endswith('.parquet')]\n",
    "\n",
    "# Ensure we have files to process\n",
    "if not parquet_files:\n",
    "    print(f\"No 'shap_*.parquet' files found in the directory '{data_directory}'.\")\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# 1. Create a 2x3 subplot grid. The figsize needs to be large enough for all charts.\n",
    "# fig is the entire figure window, axes is an array containing 6 subplot objects.\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(22, 14))\n",
    "\n",
    "# Flatten the 2D axes array into a 1D array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# --- Loop through files and plot ---\n",
    "for i, filename in enumerate(parquet_files):\n",
    "    # Stop if the number of files exceeds the number of subplots\n",
    "    if i >= 6:\n",
    "        print(\"Warning: More than 6 parquet files found. Only processing the first 6.\")\n",
    "        break\n",
    "\n",
    "    # Extract the outcome name from the filename\n",
    "    outcome_name = filename.replace('shap_', '').replace('.parquet', '')\n",
    "    full_path = os.path.join(data_directory, filename)\n",
    "    \n",
    "    print(f\"--- Processing: {filename} (Outcome: {outcome_name.upper()}) ---\")\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    try:\n",
    "        # Read the parquet file instead of csv\n",
    "        df = pd.read_parquet(full_path)\n",
    "        if 'eid' in df.columns:\n",
    "            df = df.drop(columns=['eid'])\n",
    "            print(\"  'eid' column removed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File '{full_path}' not found.\")\n",
    "        continue\n",
    "        \n",
    "    mean_abs_shap = df.abs().mean(axis=0)\n",
    "    top_30_shap = mean_abs_shap.sort_values(ascending=False).head(30)\n",
    "    \n",
    "    print(f\"  Calculated Top 30 proteins for {outcome_name.upper()}.\")\n",
    "\n",
    "    # --- Plot on the designated subplot ---\n",
    "    # 2. Select the current subplot to draw on\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # To display the most important proteins at the top, reverse the data order\n",
    "    top_30_shap.iloc[::-1].plot(kind='barh', ax=ax, color='c', zorder=2)\n",
    "    \n",
    "    # 3. Set the title and labels for each subplot\n",
    "    ax.set_title(f'Top 30 for {outcome_name.upper()}', fontsize=14, weight='bold')\n",
    "    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=10)\n",
    "    ax.set_ylabel('Protein', fontsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=8) # Adjust y-axis label font size\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7, zorder=1) # Add a grid for the x-axis\n",
    "\n",
    "# --- Post-processing ---\n",
    "# If there are fewer than 6 files, hide the remaining empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# 4. Add a main title for the entire figure\n",
    "fig.suptitle('Top 30 Metabolites by Mean Absolute SHAP Value for Each CVD Outcome', fontsize=20, weight='bold')\n",
    "\n",
    "# 5. Automatically adjust the layout to prevent overlap\n",
    "# The rect parameter makes room for the main title (suptitle)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# 6. Save and display the final figure\n",
    "# You can uncomment the next line to save the figure to a file\n",
    "# plt.savefig('combined_shap_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabolomics no statins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Reads SHAP value files from multiple runs, calculates the element-wise average,\n",
    "displays a verification sample for a single data point, and saves the final averaged SHAP values.\n",
    "\"\"\"\n",
    "# 1. Define paths\n",
    "# Base path where the SEED directories (1234, 1235, etc.) are located\n",
    "base_path = \"/your path/cardiomicscore/saved/results/SHAP/OmicsNet/Metabolomics_no_statins\"\n",
    "\n",
    "# Output path for the final averaged results\n",
    "output_path = os.path.join(base_path, \"Final\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "\n",
    "# 2. Automatically discover SEED directories and outcome files\n",
    "try:\n",
    "    # Find all directories that are named like a number (our SEEDs)\n",
    "    seed_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.isdigit()])\n",
    "    if not seed_dirs:\n",
    "        raise FileNotFoundError(\"No valid SEED directories found in the base path.\")\n",
    "    \n",
    "    print(f\"Found {len(seed_dirs)} SEED runs: {seed_dirs}\")\n",
    "\n",
    "    # Get the list of SHAP files to process from the first SEED directory\n",
    "    first_seed_path = os.path.join(base_path, seed_dirs[0])\n",
    "    shap_files_to_process = [f for f in os.listdir(first_seed_path) if f.endswith('.parquet')]\n",
    "    if not shap_files_to_process:\n",
    "        raise FileNotFoundError(f\"No .parquet files found in directory: {first_seed_path}\")\n",
    "        \n",
    "    print(f\"Found {len(shap_files_to_process)} outcomes to process: {shap_files_to_process}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 3. Loop through each SHAP file name (e.g., 'shap_af.parquet')\n",
    "for shap_filename in shap_files_to_process:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing outcome file: {shap_filename}\")\n",
    "\n",
    "    list_of_dfs = []\n",
    "    # Collect the full path for the current file from each SEED directory\n",
    "    for seed in seed_dirs:\n",
    "        file_path = os.path.join(base_path, seed, shap_filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                list_of_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file {file_path}. Error: {e}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: File not found and will be skipped: {file_path}\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(f\"No valid data found for {shap_filename}. Skipping to next outcome.\")\n",
    "        continue\n",
    "        \n",
    "    # 4. Calculate the element-wise average of the SHAP values.\n",
    "    # This is the core calculation. It produces a DataFrame with the exact same\n",
    "    # dimensions (sample size, number of proteins) as the original files.\n",
    "    final_avg_df = sum(list_of_dfs) / len(list_of_dfs)\n",
    "    print(f\"Successfully averaged {len(list_of_dfs)} files. Resulting shape: {final_avg_df.shape}\")\n",
    "\n",
    "    print(\"\\n--- Verification for a random sample and 10 random proteins ---\")\n",
    "    \n",
    "    # Check if the dataframe is empty\n",
    "    if final_avg_df.empty:\n",
    "        print(\"Averaged DataFrame is empty, skipping verification.\")\n",
    "    else:\n",
    "        # Get the number of samples (rows) and proteins (columns)\n",
    "        num_samples, num_proteins = final_avg_df.shape\n",
    "\n",
    "        # Pick one random sample (person/row) to check\n",
    "        random_sample_index = random.randint(0, num_samples - 1)\n",
    "        print(f\"Verifying with data from a single random sample (row index: {random_sample_index})\")\n",
    "\n",
    "        # Pick 10 random proteins (columns) to check\n",
    "        all_proteins = final_avg_df.columns.tolist()\n",
    "        num_to_sample = min(10, num_proteins)\n",
    "        sample_proteins = random.sample(all_proteins, num_to_sample)\n",
    "        \n",
    "        verification_data = []\n",
    "        for protein in sample_proteins:\n",
    "            row_data = {'Protein': protein}\n",
    "            \n",
    "            # Get the specific raw SHAP value from each individual run for the selected sample and protein\n",
    "            for i, df in enumerate(list_of_dfs):\n",
    "                seed_name = seed_dirs[i]\n",
    "                # Use .iloc to get the value at the specific row/column position\n",
    "                row_data[f'Run_{seed_name}_SHAP'] = df.iloc[random_sample_index][protein]\n",
    "            \n",
    "            # Get the final averaged SHAP value for the same sample and protein\n",
    "            row_data['Final_Avg_SHAP'] = final_avg_df.iloc[random_sample_index][protein]\n",
    "            verification_data.append(row_data)\n",
    "        \n",
    "        verification_df = pd.DataFrame(verification_data)\n",
    "        # Use to_string() to ensure all columns are displayed properly\n",
    "        print(verification_df.to_string())\n",
    "    \n",
    "    # 6. Save the final averaged DataFrame. The structure is already correct.\n",
    "    output_file_path = os.path.join(output_path, shap_filename)\n",
    "    try:\n",
    "        final_avg_df.to_parquet(output_file_path, engine='pyarrow')\n",
    "        print(f\"\\nSuccessfully saved final average SHAP values to:\\n{output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving file to {output_file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_directory = '/your path/cardiomicscore/saved/results/SHAP/OmicsNet/Metabolomics_no_statins/Final'\n",
    "\"\"\"\n",
    "Iterates through 'shap_*.parquet' files in a specified directory, calculates\n",
    "the mean absolute SHAP values, and plots a bar chart of the top 30 proteins\n",
    "for each file onto a 2x3 grid.\n",
    "\n",
    "Args:\n",
    "    data_directory (str): The path to the data directory containing the SHAP parquet files.\n",
    "\"\"\"\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(data_directory):\n",
    "    print(f\"Error: Directory '{data_directory}' not found.\")\n",
    "\n",
    "# Get a list of all relevant parquet files\n",
    "parquet_files = [f for f in os.listdir(data_directory) if f.startswith('shap_') and f.endswith('.parquet')]\n",
    "\n",
    "# Ensure we have files to process\n",
    "if not parquet_files:\n",
    "    print(f\"No 'shap_*.parquet' files found in the directory '{data_directory}'.\")\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# 1. Create a 2x3 subplot grid. The figsize needs to be large enough for all charts.\n",
    "# fig is the entire figure window, axes is an array containing 6 subplot objects.\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(22, 14))\n",
    "\n",
    "# Flatten the 2D axes array into a 1D array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# --- Loop through files and plot ---\n",
    "for i, filename in enumerate(parquet_files):\n",
    "    # Stop if the number of files exceeds the number of subplots\n",
    "    if i >= 6:\n",
    "        print(\"Warning: More than 6 parquet files found. Only processing the first 6.\")\n",
    "        break\n",
    "\n",
    "    # Extract the outcome name from the filename\n",
    "    outcome_name = filename.replace('shap_', '').replace('.parquet', '')\n",
    "    full_path = os.path.join(data_directory, filename)\n",
    "    \n",
    "    print(f\"--- Processing: {filename} (Outcome: {outcome_name.upper()}) ---\")\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    try:\n",
    "        # Read the parquet file instead of csv\n",
    "        df = pd.read_parquet(full_path)\n",
    "        if 'eid' in df.columns:\n",
    "            df = df.drop(columns=['eid'])\n",
    "            print(\"  'eid' column removed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File '{full_path}' not found.\")\n",
    "        continue\n",
    "        \n",
    "    mean_abs_shap = df.abs().mean(axis=0)\n",
    "    top_30_shap = mean_abs_shap.sort_values(ascending=False).head(30)\n",
    "    \n",
    "    print(f\"  Calculated Top 30 proteins for {outcome_name.upper()}.\")\n",
    "\n",
    "    # --- Plot on the designated subplot ---\n",
    "    # 2. Select the current subplot to draw on\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # To display the most important proteins at the top, reverse the data order\n",
    "    top_30_shap.iloc[::-1].plot(kind='barh', ax=ax, color='c', zorder=2)\n",
    "    \n",
    "    # 3. Set the title and labels for each subplot\n",
    "    ax.set_title(f'Top 30 for {outcome_name.upper()}', fontsize=14, weight='bold')\n",
    "    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=10)\n",
    "    ax.set_ylabel('Protein', fontsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=8) # Adjust y-axis label font size\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7, zorder=1) # Add a grid for the x-axis\n",
    "\n",
    "# --- Post-processing ---\n",
    "# If there are fewer than 6 files, hide the remaining empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# 4. Add a main title for the entire figure\n",
    "fig.suptitle('Top 30 Metabolites by Mean Absolute SHAP Value for Each CVD Outcome', fontsize=20, weight='bold')\n",
    "\n",
    "# 5. Automatically adjust the layout to prevent overlap\n",
    "# The rect parameter makes room for the main title (suptitle)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# 6. Save and display the final figure\n",
    "# You can uncomment the next line to save the figure to a file\n",
    "# plt.savefig('combined_shap_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmicsNet_Unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proteomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Reads SHAP value files from multiple runs, calculates the element-wise average,\n",
    "displays a verification sample for a single data point, and saves the final averaged SHAP values.\n",
    "\"\"\"\n",
    "# 1. Define paths\n",
    "# Base path where the SEED directories (1234, 1235, etc.) are located\n",
    "base_path = \"/your path/cardiomicscore/saved/results/SHAP/OmicsNet_Unweighted/Proteomics\"\n",
    "\n",
    "# Output path for the final averaged results\n",
    "output_path = os.path.join(base_path, \"Final\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "\n",
    "# 2. Automatically discover SEED directories and outcome files\n",
    "try:\n",
    "    # Find all directories that are named like a number (our SEEDs)\n",
    "    seed_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.isdigit()])\n",
    "    if not seed_dirs:\n",
    "        raise FileNotFoundError(\"No valid SEED directories found in the base path.\")\n",
    "    \n",
    "    print(f\"Found {len(seed_dirs)} SEED runs: {seed_dirs}\")\n",
    "\n",
    "    # Get the list of SHAP files to process from the first SEED directory\n",
    "    first_seed_path = os.path.join(base_path, seed_dirs[0])\n",
    "    shap_files_to_process = [f for f in os.listdir(first_seed_path) if f.endswith('.parquet')]\n",
    "    if not shap_files_to_process:\n",
    "        raise FileNotFoundError(f\"No .parquet files found in directory: {first_seed_path}\")\n",
    "        \n",
    "    print(f\"Found {len(shap_files_to_process)} outcomes to process: {shap_files_to_process}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 3. Loop through each SHAP file name (e.g., 'shap_af.parquet')\n",
    "for shap_filename in shap_files_to_process:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing outcome file: {shap_filename}\")\n",
    "\n",
    "    list_of_dfs = []\n",
    "    # Collect the full path for the current file from each SEED directory\n",
    "    for seed in seed_dirs:\n",
    "        file_path = os.path.join(base_path, seed, shap_filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                list_of_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file {file_path}. Error: {e}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: File not found and will be skipped: {file_path}\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(f\"No valid data found for {shap_filename}. Skipping to next outcome.\")\n",
    "        continue\n",
    "        \n",
    "    # 4. Calculate the element-wise average of the SHAP values.\n",
    "    # This is the core calculation. It produces a DataFrame with the exact same\n",
    "    # dimensions (sample size, number of proteins) as the original files.\n",
    "    final_avg_df = sum(list_of_dfs) / len(list_of_dfs)\n",
    "    print(f\"Successfully averaged {len(list_of_dfs)} files. Resulting shape: {final_avg_df.shape}\")\n",
    "\n",
    "    print(\"\\n--- Verification for a random sample and 10 random proteins ---\")\n",
    "    \n",
    "    # Check if the dataframe is empty\n",
    "    if final_avg_df.empty:\n",
    "        print(\"Averaged DataFrame is empty, skipping verification.\")\n",
    "    else:\n",
    "        # Get the number of samples (rows) and proteins (columns)\n",
    "        num_samples, num_proteins = final_avg_df.shape\n",
    "\n",
    "        # Pick one random sample (person/row) to check\n",
    "        random_sample_index = random.randint(0, num_samples - 1)\n",
    "        print(f\"Verifying with data from a single random sample (row index: {random_sample_index})\")\n",
    "\n",
    "        # Pick 10 random proteins (columns) to check\n",
    "        all_proteins = final_avg_df.columns.tolist()\n",
    "        num_to_sample = min(10, num_proteins)\n",
    "        sample_proteins = random.sample(all_proteins, num_to_sample)\n",
    "        \n",
    "        verification_data = []\n",
    "        for protein in sample_proteins:\n",
    "            row_data = {'Protein': protein}\n",
    "            \n",
    "            # Get the specific raw SHAP value from each individual run for the selected sample and protein\n",
    "            for i, df in enumerate(list_of_dfs):\n",
    "                seed_name = seed_dirs[i]\n",
    "                # Use .iloc to get the value at the specific row/column position\n",
    "                row_data[f'Run_{seed_name}_SHAP'] = df.iloc[random_sample_index][protein]\n",
    "            \n",
    "            # Get the final averaged SHAP value for the same sample and protein\n",
    "            row_data['Final_Avg_SHAP'] = final_avg_df.iloc[random_sample_index][protein]\n",
    "            verification_data.append(row_data)\n",
    "        \n",
    "        verification_df = pd.DataFrame(verification_data)\n",
    "        # Use to_string() to ensure all columns are displayed properly\n",
    "        print(verification_df.to_string())\n",
    "    \n",
    "    # 6. Save the final averaged DataFrame. The structure is already correct.\n",
    "    output_file_path = os.path.join(output_path, shap_filename)\n",
    "    try:\n",
    "        final_avg_df.to_parquet(output_file_path, engine='pyarrow')\n",
    "        print(f\"\\nSuccessfully saved final average SHAP values to:\\n{output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving file to {output_file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_directory = '/your path/cardiomicscore/saved/results/SHAP/OmicsNet_Unweighted/Proteomics/Final'\n",
    "\"\"\"\n",
    "Iterates through 'shap_*.parquet' files in a specified directory, calculates\n",
    "the mean absolute SHAP values, and plots a bar chart of the top 30 proteins\n",
    "for each file onto a 2x3 grid.\n",
    "\n",
    "Args:\n",
    "    data_directory (str): The path to the data directory containing the SHAP parquet files.\n",
    "\"\"\"\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(data_directory):\n",
    "    print(f\"Error: Directory '{data_directory}' not found.\")\n",
    "\n",
    "# Get a list of all relevant parquet files\n",
    "parquet_files = [f for f in os.listdir(data_directory) if f.startswith('shap_') and f.endswith('.parquet')]\n",
    "\n",
    "# Ensure we have files to process\n",
    "if not parquet_files:\n",
    "    print(f\"No 'shap_*.parquet' files found in the directory '{data_directory}'.\")\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# 1. Create a 2x3 subplot grid. The figsize needs to be large enough for all charts.\n",
    "# fig is the entire figure window, axes is an array containing 6 subplot objects.\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(22, 14))\n",
    "\n",
    "# Flatten the 2D axes array into a 1D array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# --- Loop through files and plot ---\n",
    "for i, filename in enumerate(parquet_files):\n",
    "    # Stop if the number of files exceeds the number of subplots\n",
    "    if i >= 6:\n",
    "        print(\"Warning: More than 6 parquet files found. Only processing the first 6.\")\n",
    "        break\n",
    "\n",
    "    # Extract the outcome name from the filename\n",
    "    outcome_name = filename.replace('shap_', '').replace('.parquet', '')\n",
    "    full_path = os.path.join(data_directory, filename)\n",
    "    \n",
    "    print(f\"--- Processing: {filename} (Outcome: {outcome_name.upper()}) ---\")\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    try:\n",
    "        # Read the parquet file instead of csv\n",
    "        df = pd.read_parquet(full_path)\n",
    "        if 'eid' in df.columns:\n",
    "            df = df.drop(columns=['eid'])\n",
    "            print(\"  'eid' column removed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File '{full_path}' not found.\")\n",
    "        continue\n",
    "        \n",
    "    mean_abs_shap = df.abs().mean(axis=0)\n",
    "    top_30_shap = mean_abs_shap.sort_values(ascending=False).head(30)\n",
    "    \n",
    "    print(f\"  Calculated Top 30 proteins for {outcome_name.upper()}.\")\n",
    "\n",
    "    # --- Plot on the designated subplot ---\n",
    "    # 2. Select the current subplot to draw on\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # To display the most important proteins at the top, reverse the data order\n",
    "    top_30_shap.iloc[::-1].plot(kind='barh', ax=ax, color='c', zorder=2)\n",
    "    \n",
    "    # 3. Set the title and labels for each subplot\n",
    "    ax.set_title(f'Top 30 for {outcome_name.upper()}', fontsize=14, weight='bold')\n",
    "    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=10)\n",
    "    ax.set_ylabel('Protein', fontsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=8) # Adjust y-axis label font size\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7, zorder=1) # Add a grid for the x-axis\n",
    "\n",
    "# --- Post-processing ---\n",
    "# If there are fewer than 6 files, hide the remaining empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# 4. Add a main title for the entire figure\n",
    "fig.suptitle('Top 30 Metabolites by Mean Absolute SHAP Value for Each CVD Outcome', fontsize=20, weight='bold')\n",
    "\n",
    "# 5. Automatically adjust the layout to prevent overlap\n",
    "# The rect parameter makes room for the main title (suptitle)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# 6. Save and display the final figure\n",
    "# You can uncomment the next line to save the figure to a file\n",
    "# plt.savefig('combined_shap_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabolomics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "Reads SHAP value files from multiple runs, calculates the element-wise average,\n",
    "displays a verification sample for a single data point, and saves the final averaged SHAP values.\n",
    "\"\"\"\n",
    "# 1. Define paths\n",
    "# Base path where the SEED directories (1234, 1235, etc.) are located\n",
    "base_path = \"/your path/cardiomicscore/saved/results/SHAP/OmicsNet_Unweighted/Metabolomics\"\n",
    "\n",
    "# Output path for the final averaged results\n",
    "output_path = os.path.join(base_path, \"Final\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Results will be saved to: {output_path}\")\n",
    "\n",
    "# 2. Automatically discover SEED directories and outcome files\n",
    "try:\n",
    "    # Find all directories that are named like a number (our SEEDs)\n",
    "    seed_dirs = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d)) and d.isdigit()])\n",
    "    if not seed_dirs:\n",
    "        raise FileNotFoundError(\"No valid SEED directories found in the base path.\")\n",
    "    \n",
    "    print(f\"Found {len(seed_dirs)} SEED runs: {seed_dirs}\")\n",
    "\n",
    "    # Get the list of SHAP files to process from the first SEED directory\n",
    "    first_seed_path = os.path.join(base_path, seed_dirs[0])\n",
    "    shap_files_to_process = [f for f in os.listdir(first_seed_path) if f.endswith('.parquet')]\n",
    "    if not shap_files_to_process:\n",
    "        raise FileNotFoundError(f\"No .parquet files found in directory: {first_seed_path}\")\n",
    "        \n",
    "    print(f\"Found {len(shap_files_to_process)} outcomes to process: {shap_files_to_process}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# 3. Loop through each SHAP file name (e.g., 'shap_af.parquet')\n",
    "for shap_filename in shap_files_to_process:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Processing outcome file: {shap_filename}\")\n",
    "\n",
    "    list_of_dfs = []\n",
    "    # Collect the full path for the current file from each SEED directory\n",
    "    for seed in seed_dirs:\n",
    "        file_path = os.path.join(base_path, seed, shap_filename)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                list_of_dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read file {file_path}. Error: {e}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"Warning: File not found and will be skipped: {file_path}\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(f\"No valid data found for {shap_filename}. Skipping to next outcome.\")\n",
    "        continue\n",
    "        \n",
    "    # 4. Calculate the element-wise average of the SHAP values.\n",
    "    # This is the core calculation. It produces a DataFrame with the exact same\n",
    "    # dimensions (sample size, number of proteins) as the original files.\n",
    "    final_avg_df = sum(list_of_dfs) / len(list_of_dfs)\n",
    "    print(f\"Successfully averaged {len(list_of_dfs)} files. Resulting shape: {final_avg_df.shape}\")\n",
    "\n",
    "    print(\"\\n--- Verification for a random sample and 10 random proteins ---\")\n",
    "    \n",
    "    # Check if the dataframe is empty\n",
    "    if final_avg_df.empty:\n",
    "        print(\"Averaged DataFrame is empty, skipping verification.\")\n",
    "    else:\n",
    "        # Get the number of samples (rows) and proteins (columns)\n",
    "        num_samples, num_proteins = final_avg_df.shape\n",
    "\n",
    "        # Pick one random sample (person/row) to check\n",
    "        random_sample_index = random.randint(0, num_samples - 1)\n",
    "        print(f\"Verifying with data from a single random sample (row index: {random_sample_index})\")\n",
    "\n",
    "        # Pick 10 random proteins (columns) to check\n",
    "        all_proteins = final_avg_df.columns.tolist()\n",
    "        num_to_sample = min(10, num_proteins)\n",
    "        sample_proteins = random.sample(all_proteins, num_to_sample)\n",
    "        \n",
    "        verification_data = []\n",
    "        for protein in sample_proteins:\n",
    "            row_data = {'Protein': protein}\n",
    "            \n",
    "            # Get the specific raw SHAP value from each individual run for the selected sample and protein\n",
    "            for i, df in enumerate(list_of_dfs):\n",
    "                seed_name = seed_dirs[i]\n",
    "                # Use .iloc to get the value at the specific row/column position\n",
    "                row_data[f'Run_{seed_name}_SHAP'] = df.iloc[random_sample_index][protein]\n",
    "            \n",
    "            # Get the final averaged SHAP value for the same sample and protein\n",
    "            row_data['Final_Avg_SHAP'] = final_avg_df.iloc[random_sample_index][protein]\n",
    "            verification_data.append(row_data)\n",
    "        \n",
    "        verification_df = pd.DataFrame(verification_data)\n",
    "        # Use to_string() to ensure all columns are displayed properly\n",
    "        print(verification_df.to_string())\n",
    "    \n",
    "    # 6. Save the final averaged DataFrame. The structure is already correct.\n",
    "    output_file_path = os.path.join(output_path, shap_filename)\n",
    "    try:\n",
    "        final_avg_df.to_parquet(output_file_path, engine='pyarrow')\n",
    "        print(f\"\\nSuccessfully saved final average SHAP values to:\\n{output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving file to {output_file_path}. Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean absolute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_directory = '/your path/cardiomicscore/saved/results/SHAP/OmicsNet_Unweighted/Metabolomics/Final'\n",
    "\"\"\"\n",
    "Iterates through 'shap_*.parquet' files in a specified directory, calculates\n",
    "the mean absolute SHAP values, and plots a bar chart of the top 30 proteins\n",
    "for each file onto a 2x3 grid.\n",
    "\n",
    "Args:\n",
    "    data_directory (str): The path to the data directory containing the SHAP parquet files.\n",
    "\"\"\"\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(data_directory):\n",
    "    print(f\"Error: Directory '{data_directory}' not found.\")\n",
    "\n",
    "# Get a list of all relevant parquet files\n",
    "parquet_files = [f for f in os.listdir(data_directory) if f.startswith('shap_') and f.endswith('.parquet')]\n",
    "\n",
    "# Ensure we have files to process\n",
    "if not parquet_files:\n",
    "    print(f\"No 'shap_*.parquet' files found in the directory '{data_directory}'.\")\n",
    "\n",
    "# --- Plotting Setup ---\n",
    "# 1. Create a 2x3 subplot grid. The figsize needs to be large enough for all charts.\n",
    "# fig is the entire figure window, axes is an array containing 6 subplot objects.\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(22, 14))\n",
    "\n",
    "# Flatten the 2D axes array into a 1D array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# --- Loop through files and plot ---\n",
    "for i, filename in enumerate(parquet_files):\n",
    "    # Stop if the number of files exceeds the number of subplots\n",
    "    if i >= 6:\n",
    "        print(\"Warning: More than 6 parquet files found. Only processing the first 6.\")\n",
    "        break\n",
    "\n",
    "    # Extract the outcome name from the filename\n",
    "    outcome_name = filename.replace('shap_', '').replace('.parquet', '')\n",
    "    full_path = os.path.join(data_directory, filename)\n",
    "    \n",
    "    print(f\"--- Processing: {filename} (Outcome: {outcome_name.upper()}) ---\")\n",
    "\n",
    "    # --- Data Loading and Processing ---\n",
    "    try:\n",
    "        # Read the parquet file instead of csv\n",
    "        df = pd.read_parquet(full_path)\n",
    "        if 'eid' in df.columns:\n",
    "            df = df.drop(columns=['eid'])\n",
    "            print(\"  'eid' column removed.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  Error: File '{full_path}' not found.\")\n",
    "        continue\n",
    "        \n",
    "    mean_abs_shap = df.abs().mean(axis=0)\n",
    "    top_30_shap = mean_abs_shap.sort_values(ascending=False).head(30)\n",
    "    \n",
    "    print(f\"  Calculated Top 30 proteins for {outcome_name.upper()}.\")\n",
    "\n",
    "    # --- Plot on the designated subplot ---\n",
    "    # 2. Select the current subplot to draw on\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # To display the most important proteins at the top, reverse the data order\n",
    "    top_30_shap.iloc[::-1].plot(kind='barh', ax=ax, color='c', zorder=2)\n",
    "    \n",
    "    # 3. Set the title and labels for each subplot\n",
    "    ax.set_title(f'Top 30 for {outcome_name.upper()}', fontsize=14, weight='bold')\n",
    "    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=10)\n",
    "    ax.set_ylabel('Protein', fontsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=8) # Adjust y-axis label font size\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7, zorder=1) # Add a grid for the x-axis\n",
    "\n",
    "# --- Post-processing ---\n",
    "# If there are fewer than 6 files, hide the remaining empty subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# 4. Add a main title for the entire figure\n",
    "fig.suptitle('Top 30 Metabolites by Mean Absolute SHAP Value for Each CVD Outcome', fontsize=20, weight='bold')\n",
    "\n",
    "# 5. Automatically adjust the layout to prevent overlap\n",
    "# The rect parameter makes room for the main title (suptitle)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "# 6. Save and display the final figure\n",
    "# You can uncomment the next line to save the figure to a file\n",
    "# plt.savefig('combined_shap_analysis.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll files processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omicscvd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
